{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpt_vision_demo.py -- James Sayre, jsayre@ucdavis.edu\n",
    "\n",
    "##### This code demonstrates use of OpenAI's API to process imagery and extract structured data. It extracts an image from a PDF, then shows how one can use on-demand (quick, but more expensive) and batch processing (slow, but cheaper) functionalities to extract data to a pre-defined file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "### For handling PDF and image files\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "\n",
    "### Params\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY') # Get your OpenAI API key from environment variables\n",
    "\n",
    "### Directories\n",
    "### Define your own directories here\n",
    "\n",
    "### Inputs\n",
    "test_doc     =  'test_doc.pdf'\n",
    "\n",
    "### Parameters\n",
    "### Prompt engineering\n",
    "# Define expected fields for structured output\n",
    "str_functions = [\n",
    "    {\n",
    "        \"name\": \"extract_soil_certificate_info\", ### Need to provide name\n",
    "        \"description\": \"Extracts data from soil certificate\", ### Description for function\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                ### \n",
    "                \"predominant_soil\": {\"type\": \"string\", \"description\": \"The predominant soil.\"},\n",
    "                \"crop\": {\"type\": \"string\", \"description\": \"Crop.\"},\n",
    "                \"baseline_township\": {\"type\": \"string\", \"description\": \"Region hihglighted in the image under Baseline, Township\"},\n",
    "                \"range_south\": {\"type\": \"string\", \"description\": \"degrees south\"},\n",
    "                \"range_east\": {\"type\": \"string\", \"description\": \"degrees east\"},\n",
    "                \"practice\": {\"type\": \"string\", \"description\": \"Category highlighted in the image under Practice\"}\n",
    "            },\n",
    "            ### This tells GPT4 to always output the following columns/info\n",
    "            \"required\": [\"predominant_soil\", \"crop\", \"baseline_township\", \"range_south\", \"range_east\", \"practice\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prompt text with your custom message\n",
    "prompt_text = '''You are SoilGPT, and I want you to read an image.'''\n",
    "prompt_text += ''' Please extract the value in each field outlined by a partial rectangle in the image.'''\n",
    "\n",
    "### Programs\n",
    "### Take a page of PDF, check for text string in it, and return cropped image as base64 object\n",
    "def imagetize_pdf(pdf_doc, page_num, check_for_text=\"\", save_png=False):\n",
    "    first_page = pdf_doc.load_page(page_num)\n",
    "    if check_for_text != \"\":\n",
    "        if check_for_text not in first_page.get_text(\"text\"):\n",
    "            return None\n",
    "        \n",
    "    # Get dimensions of PDF\n",
    "    original_width = first_page.rect.width\n",
    "    original_height = first_page.rect.height\n",
    "\n",
    "    # Calculate cropping dimensions\n",
    "    top_crop = original_height * (3/12)\n",
    "    bottom_crop = original_height * (8/20)\n",
    "    left_crop = original_width * (1.5/20)\n",
    "    right_crop = original_width * (11.5/20)\n",
    "\n",
    "    # Crop the page\n",
    "    cropped_rect = fitz.Rect(left_crop, top_crop, right_crop, bottom_crop)\n",
    "    zoom = 3  # Set the zoom factor for higher resolution\n",
    "    matrix = fitz.Matrix(zoom, zoom)\n",
    "    cropped_image = first_page.get_pixmap(matrix=matrix, clip=cropped_rect)\n",
    "\n",
    "    # Convert to PIL Image and save as PNG\n",
    "    img = Image.frombytes(\"RGB\", [cropped_image.width, cropped_image.height], cropped_image.samples)\n",
    "    ### save image to file\n",
    "    if save_png:\n",
    "        img.save(f\"page_{page_num}.png\")\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    \n",
    "\n",
    "    base64_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{base64_image}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to make calls to the OpenAI API. The first is to make immeditate calls to the API. This method is the simplest, as you will make a call, wait for the response from server, and immediately store the output. However, you will pay more for the privilege of receiving on-demand output. Code to implement the first is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to analyze the information in the image file based on a prompt and structured output information\n",
    "def analyze_gpt_text(base_img, prompt, structured_functions):\n",
    "    if base_img is None:\n",
    "        return \"No image generated. Check PDF text and page number.\"\n",
    "\n",
    "    img_data = base64.b64decode(base_img.split(\",\")[1])\n",
    "    with open(\"debug_cropped_image.png\", \"wb\") as f:\n",
    "        f.write(img_data)  # Save to inspect\n",
    "\n",
    "    # Define the content with text information (without the function call)\n",
    "    content = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "\n",
    "    img_content = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": base_img\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    # Create the response using OpenAI's client\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[content,img_content],\n",
    "        functions=structured_functions,\n",
    "        function_call={\"name\": \"extract_soil_certificate_info\"}  # Specify the function here\n",
    "    )\n",
    "\n",
    "    # Parse the structured output\n",
    "    try:\n",
    "        # Access the function's output arguments\n",
    "        structured_output = response.choices[0].message.function_call.arguments\n",
    "        data_dict = json.loads(structured_output)  # Convert JSON string to dictionary\n",
    "        df = pd.DataFrame([data_dict])  # Convert to DataFrame\n",
    "        return df\n",
    "    except (IndexError, KeyError, json.JSONDecodeError) as e:\n",
    "        return f\"Error parsing response: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document  =  fitz.open(test_doc)\n",
    "chatgptoutput =  analyze_gpt_text(imagetize_pdf(pdf_document, 17), prompt_text, str_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've run this code, we can confirm that the output is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predominant_soil</th>\n",
       "      <th>crop</th>\n",
       "      <th>baseline_township</th>\n",
       "      <th>range_south</th>\n",
       "      <th>range_east</th>\n",
       "      <th>practice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loamy Sand</td>\n",
       "      <td>Squash</td>\n",
       "      <td>Mt. Diablo</td>\n",
       "      <td>17S</td>\n",
       "      <td>23E</td>\n",
       "      <td>SURFACE IRRIGATION (Without a tailwater recove...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predominant_soil    crop baseline_township range_south range_east  \\\n",
       "0       Loamy Sand  Squash        Mt. Diablo         17S        23E   \n",
       "\n",
       "                                            practice  \n",
       "0  SURFACE IRRIGATION (Without a tailwater recove...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgptoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to make batch calls to the OpenAI API. These will upload the images or text you intend to process as a batch, which will be computed within 24 hours (often much less). The upshot of this method is both that you don't have to have a long running script calling the API open on your computer for a while as well as the reduced cost of batch processing vis-a-vis on-demand processing. However, the documentation for batch processing is much sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Function to create a JSONL entry for a given page with structured output request\n",
    "def create_jsonl_entry(page_num, base_img, prompt, structured_functions):\n",
    "    content = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "\n",
    "    img_content = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": base_img\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    return {\n",
    "        \"custom_id\": f\"task-{page_num}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            # This is what you would have in your Chat Completions API call\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0,\n",
    "            \"response_format\": { \n",
    "                \"type\": \"json_object\"\n",
    "            },\n",
    "            \"messages\": [content,img_content],\n",
    "            \"functions\":structured_functions,\n",
    "            \"function_call\":{\"name\": \"extract_soil_certificate_info\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to generate JSONL file for batch processing with structured output\n",
    "def generate_jsonl_file(pdf_path, page_numbers, prompt, structured_functions, output_file=\"input.jsonl\"):\n",
    "    entries = []\n",
    "    for page_num in page_numbers:\n",
    "        base_img = imagetize_pdf(pdf_path, page_num)\n",
    "        if base_img:\n",
    "            entry = create_jsonl_entry(page_num, base_img, prompt, structured_functions)\n",
    "            entries.append(entry)\n",
    "    \n",
    "    # Write entries to JSONL file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for entry in entries:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    print(f\"JSONL file created: {output_file}\")\n",
    "\n",
    "def upload_jsonl_file(file_path):\n",
    "    response = openai.files.create(\n",
    "        file=open(file_path, \"rb\"),\n",
    "        purpose=\"batch\"  # Adjust purpose if needed; in some cases, \"answers\" might be appropriate,\n",
    "    )\n",
    "    file_id = response.id\n",
    "    print(f\"File uploaded successfully. File ID: {file_id}\")\n",
    "    return file_id\n",
    "\n",
    "# Function to create a batch job\n",
    "def create_batch_job(file_id, model=\"gpt-4o\", completion_window=\"24h\"):\n",
    "    response = openai.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        # model=model,\n",
    "        completion_window=completion_window,\n",
    "        metadata={\"description\": \"Batch processing for soil certificate analysis\"}\n",
    "    )\n",
    "    # batch_id = response['id']\n",
    "    # print(f\"Batch job created successfully. Batch ID: {batch_id}\")\n",
    "    # return batch_id\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a batch on two pages of this pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document  =  fitz.open(test_doc)\n",
    "\n",
    "jsonl_file_path = \"input.jsonl\" # Define the JSONL file path\n",
    "generate_jsonl_file(pdf_document, [17,18], prompt_text, str_functions, jsonl_file_path)\n",
    "file_id = upload_jsonl_file(jsonl_file_path)  # Upload JSONL file\n",
    "batch_id = create_batch_job(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we can check the status of all the batch jobs submitted\n",
    "openai.batches.list(limit=10)\n",
    "### Or check the status of a specific batch job\n",
    "batch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, then retrieve the contents of the batch file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = openai.files.content(batch_id.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
